{"cells":[{"source":"import sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter('ignore')","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1703860904810,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter('ignore')"},"id":"960b5b5e-bd91-44a8-9d7b-2cc51b6c1392","cell_type":"code","execution_count":25,"outputs":[]},{"source":"import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom tqdm import tqdm\nsns.set()\ntf.compat.v1.random.set_random_seed(1234)","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1703860904863,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom tqdm import tqdm\nsns.set()\ntf.compat.v1.random.set_random_seed(1234)"},"id":"9aee8a6c-b2b8-45b2-9569-41baf454a5b6","cell_type":"code","execution_count":26,"outputs":[]},{"source":"# Load and Prepare Data\ndf = pd.read_csv(\"main_data.csv\")\ndf.fillna(0, inplace=True)\ndf.head()","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1703860904912,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load and Prepare Data\ndf = pd.read_csv(\"main_data.csv\")\ndf.fillna(0, inplace=True)\ndf.head()","outputsMetadata":{"0":{"height":209,"type":"dataFrame"}}},"id":"2e93dee6-eba7-4b21-9261-07020c70f48f","cell_type":"code","execution_count":27,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"date","type":"string"},{"name":"vcb_close","type":"number"},{"name":"vnindex_close","type":"number"},{"name":"exchange_rate","type":"number"},{"name":"interest_rate","type":"number"},{"name":"gdp","type":"number"},{"name":"inflation","type":"number"},{"name":"sp500_close","type":"number"},{"name":"gdp_growth","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"date":["2019-12-30","2019-12-31","2020-01-02","2020-01-03","2020-01-06"],"vcb_close":[69.881,69.267,69.728,69.037,67.194],"vnindex_close":[965.03,960.99,966.67,965.14,955.79],"exchange_rate":[23167,23155,23150,23157,23167],"interest_rate":[1.3,1.43,2.44,1.99,1.58],"gdp":[2010887,2010887,1188207,1188207,1188207],"inflation":[1.4,1.4,1.23,1.23,1.23],"sp500_close":[3221.29,3230.78,3257.85,3234.85,3246.28],"gdp_growth":[0,0,-0.4091129934,0,0]}},"total_rows":5,"truncation_type":null},"text/plain":"         date  vcb_close  vnindex_close  ...  inflation  sp500_close  gdp_growth\n0  2019-12-30     69.881         965.03  ...       1.40      3221.29    0.000000\n1  2019-12-31     69.267         960.99  ...       1.40      3230.78    0.000000\n2  2020-01-02     69.728         966.67  ...       1.23      3257.85   -0.409113\n3  2020-01-03     69.037         965.14  ...       1.23      3234.85    0.000000\n4  2020-01-06     67.194         955.79  ...       1.23      3246.28    0.000000\n\n[5 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>vcb_close</th>\n      <th>vnindex_close</th>\n      <th>exchange_rate</th>\n      <th>interest_rate</th>\n      <th>gdp</th>\n      <th>inflation</th>\n      <th>sp500_close</th>\n      <th>gdp_growth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-12-30</td>\n      <td>69.881</td>\n      <td>965.03</td>\n      <td>23167.0</td>\n      <td>1.30</td>\n      <td>2010887.0</td>\n      <td>1.40</td>\n      <td>3221.29</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-12-31</td>\n      <td>69.267</td>\n      <td>960.99</td>\n      <td>23155.0</td>\n      <td>1.43</td>\n      <td>2010887.0</td>\n      <td>1.40</td>\n      <td>3230.78</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-01-02</td>\n      <td>69.728</td>\n      <td>966.67</td>\n      <td>23150.0</td>\n      <td>2.44</td>\n      <td>1188207.0</td>\n      <td>1.23</td>\n      <td>3257.85</td>\n      <td>-0.409113</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-01-03</td>\n      <td>69.037</td>\n      <td>965.14</td>\n      <td>23157.0</td>\n      <td>1.99</td>\n      <td>1188207.0</td>\n      <td>1.23</td>\n      <td>3234.85</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-01-06</td>\n      <td>67.194</td>\n      <td>955.79</td>\n      <td>23167.0</td>\n      <td>1.58</td>\n      <td>1188207.0</td>\n      <td>1.23</td>\n      <td>3246.28</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":27}]},{"source":"minmax = MinMaxScaler().fit(df[['vnindex_close', 'sp500_close', 'gdp_growth', 'exchange_rate','interest_rate','inflation']].astype('float32')) # Close index\ndf_log = minmax.transform(df[['vnindex_close', 'sp500_close', 'gdp_growth', 'exchange_rate','interest_rate','inflation']].astype('float32')) # Close index\ndf_log = pd.DataFrame(df_log)\ndf_log.head()","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1703860904964,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"minmax = MinMaxScaler().fit(df[['vnindex_close', 'sp500_close', 'gdp_growth', 'exchange_rate','interest_rate','inflation']].astype('float32')) # Close index\ndf_log = minmax.transform(df[['vnindex_close', 'sp500_close', 'gdp_growth', 'exchange_rate','interest_rate','inflation']].astype('float32')) # Close index\ndf_log = pd.DataFrame(df_log)\ndf_log.head()","outputsMetadata":{"0":{"height":209,"type":"dataFrame"}}},"id":"f9fb4063-2b2c-4874-a8e6-ea9fb58afd2a","cell_type":"code","execution_count":28,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"0","type":"number"},{"name":"1","type":"number"},{"name":"2","type":"number"},{"name":"3","type":"number"},{"name":"4","type":"number"},{"name":"5","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"0":[0.3517760038,0.3471288681,0.3536624908,0.3519026041,0.3411474228],"1":[0.3849123716,0.3886250257,0.3992152214,0.3902173042,0.3946888447],"2":[0.5526060462,0.5526060462,0,0.5526060462,0.5526060462],"3":[0.1480484009,0.1318969727,0.1251678467,0.1345901489,0.1480484009],"4":[0.1438848972,0.1594724208,0.2805755734,0.2266187221,0.1774580479],"5":[0.9607843161,0.9607843161,0.9052287936,0.9052287936,0.9052287936],"index":[0,1,2,3,4]}},"total_rows":5,"truncation_type":null},"text/plain":"          0         1         2         3         4         5\n0  0.351776  0.384912  0.552606  0.148048  0.143885  0.960784\n1  0.347129  0.388625  0.552606  0.131897  0.159472  0.960784\n2  0.353662  0.399215  0.000000  0.125168  0.280576  0.905229\n3  0.351903  0.390217  0.552606  0.134590  0.226619  0.905229\n4  0.341147  0.394689  0.552606  0.148048  0.177458  0.905229","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.351776</td>\n      <td>0.384912</td>\n      <td>0.552606</td>\n      <td>0.148048</td>\n      <td>0.143885</td>\n      <td>0.960784</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.347129</td>\n      <td>0.388625</td>\n      <td>0.552606</td>\n      <td>0.131897</td>\n      <td>0.159472</td>\n      <td>0.960784</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.353662</td>\n      <td>0.399215</td>\n      <td>0.000000</td>\n      <td>0.125168</td>\n      <td>0.280576</td>\n      <td>0.905229</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.351903</td>\n      <td>0.390217</td>\n      <td>0.552606</td>\n      <td>0.134590</td>\n      <td>0.226619</td>\n      <td>0.905229</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.341147</td>\n      <td>0.394689</td>\n      <td>0.552606</td>\n      <td>0.148048</td>\n      <td>0.177458</td>\n      <td>0.905229</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":28}]},{"source":"test_size = 600\nsimulation_size = 10\n\ndf_train = df_log.iloc[:-test_size]\ndf_test = df_log.iloc[-test_size:]\ndf.shape, df_train.shape, df_test.shape\n\nclass Model(tf.keras.Model):\n    def __init__(\n        self,\n        size,\n        size_layer,\n        output_size,\n        num_layers,\n        forget_bias=0.1,\n        learning_rate=0.001,\n    ):\n        super(Model, self).__init__()\n        self.num_layers = num_layers\n        self.lstm_layers = [tf.keras.layers.LSTM(size_layer, return_sequences=True) for _ in range(num_layers)]\n        self.rnn = tf.keras.Sequential(self.lstm_layers)\n        self.out = tf.keras.layers.Dense(output_size)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n        self.forget_bias = forget_bias\n\n    def call(self, inputs, training=False):\n        x = self.rnn(inputs)\n        return self.out(x)\n\ndef calculate_accuracy(real, predict):\n    return 1 - np.sqrt(np.mean(np.square((real - predict) / real))) * 100\n\ndef anchor(signal, weight):\n    buffer = []\n    last = signal[0]\n    for i in signal:\n        smoothed_val = last * weight + (1 - weight) * i\n        buffer.append(smoothed_val)\n        last = smoothed_val\n    return buffer","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1703860905016,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"test_size = 600\nsimulation_size = 10\n\ndf_train = df_log.iloc[:-test_size]\ndf_test = df_log.iloc[-test_size:]\ndf.shape, df_train.shape, df_test.shape"},"id":"5327a1b2-4e32-4908-80e8-e61b4502a412","cell_type":"code","execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":"((874, 9), (274, 6), (600, 6))"},"metadata":{},"execution_count":29}]},{"source":"class Model(tf.keras.Model):\n    def __init__(\n        self,\n        size,\n        size_layer,\n        output_size,\n        num_layers,\n        forget_bias=0.1,\n        learning_rate=0.001,\n    ):\n        super(Model, self).__init__()\n        self.num_layers = num_layers\n        self.lstm_layers = [tf.keras.layers.LSTM(size_layer, return_sequences=True) for _ in range(num_layers)]\n        self.rnn = tf.keras.Sequential(self.lstm_layers)\n        self.out = tf.keras.layers.Dense(output_size)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n        self.forget_bias = forget_bias\n\n    def call(self, inputs, training=False):\n        x = self.rnn(inputs)\n        return self.out(x)\n\ndef calculate_accuracy(real, predict):\n    return 1 - np.sqrt(np.mean(np.square((real - predict) / real))) * 100\n\ndef anchor(signal, weight):\n    buffer = []\n    last = signal[0]\n    for i in signal:\n        smoothed_val = last * weight + (1 - weight) * i\n        buffer.append(smoothed_val)\n        last = smoothed_val\n    return buffer\n \nnum_layers = 1\nsize_layer = 128\ntimestamp = 6\nepoch = 300\ndropout_rate = 0.8\nfuture_day = test_size\nlearning_rate = 0.01\n\ndef forecast():\n    model = Model(learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate)\n    minmax = MinMaxScaler().fit(df_log)\n    df_train = minmax.transform(df_log)\n    \n    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n\n    pbar = tqdm(range(epoch), desc='train loop')\n    for i in pbar:\n        init_value = np.zeros((1, num_layers * size_layer))\n        total_loss, total_acc = [], []\n        for k in range(0, df_train.shape[0] - 1, timestamp):\n            index = min(k + timestamp, df_train.shape[0] - 1)\n            batch_x = np.expand_dims(df_train[k:index, :], axis=0)\n            batch_y = df_train[k + 1 : index + 1, :]\n            with tf.GradientTape() as tape:\n                logits = model(batch_x)\n                loss = tf.reduce_mean(tf.square(batch_y - logits))\n            gradients = tape.gradient(loss, model.trainable_variables)\n            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n            total_loss.append(loss.numpy())\n            total_acc.append(calculate_accuracy(batch_y, logits))\n        pbar.set_postfix(cost=np.mean(total_loss), acc=np.mean(total_acc))\n    \n    future_day = test_size\n\n    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n    output_predict[0] = df_train.iloc[0]\n    upper_b = (df_train.shape[0] // timestamp) * timestamp\n    init_value = np.zeros((1, num_layers * 2 * size_layer))\n\n    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n        out_logits, last_state = sess.run(\n            [modelnn.logits, modelnn.last_state],\n            feed_dict = {\n                modelnn.X: np.expand_dims(\n                    df_train.iloc[k : k + timestamp], axis = 0\n                ),\n                modelnn.hidden_layer: init_value,\n            },\n        )\n        init_value = last_state\n        output_predict[k + 1 : k + timestamp + 1] = out_logits\n\n    if upper_b != df_train.shape[0]:\n        out_logits, last_state = sess.run(\n            [modelnn.logits, modelnn.last_state],\n            feed_dict = {\n                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n                modelnn.hidden_layer: init_value,\n            },\n        )\n        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n        future_day -= 1\n        date_ori.append(date_ori[-1] + timedelta(days = 1))\n\n    init_value = last_state\n    \n    for i in range(future_day):\n        o = output_predict[-future_day - timestamp + i:-future_day + i]\n        out_logits, last_state = sess.run(\n            [model.rnn, model.last_state],\n            feed_dict = {\n                model.X: np.expand_dims(o, axis = 0),\n                model.hidden_layer: init_value,\n            },\n        )\n        init_value = last_state\n        output_predict[-future_day + i] = out_logits[-1]\n        date_ori.append(date_ori[-1] + timedelta(days = 1))\n    \n    output_predict = minmax.inverse_transform(output_predict)\n    deep_future = anchor(output_predict, 0.3)\n    \n    return deep_future[-test_size:]\n\nresults = []\nfor i in range(simulation_size):\n    print('simulation %d'%(i + 1))\n    results.append(forecast())","metadata":{"executionCancelledAt":null,"executionTime":55154831,"lastExecutedAt":1703917435988,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class Model(tf.keras.Model):\n    def __init__(\n        self,\n        size,\n        size_layer,\n        output_size,\n        num_layers,\n        forget_bias=0.1,\n        learning_rate=0.001,\n    ):\n        super(Model, self).__init__()\n        self.num_layers = num_layers\n        self.lstm_layers = [tf.keras.layers.LSTM(size_layer, return_sequences=True) for _ in range(num_layers)]\n        self.rnn = tf.keras.Sequential(self.lstm_layers)\n        self.out = tf.keras.layers.Dense(output_size)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n        self.forget_bias = forget_bias\n\n    def call(self, inputs, training=False):\n        x = self.rnn(inputs)\n        return self.out(x)\n\ndef calculate_accuracy(real, predict):\n    return 1 - np.sqrt(np.mean(np.square((real - predict) / real))) * 100\n\ndef anchor(signal, weight):\n    buffer = []\n    last = signal[0]\n    for i in signal:\n        smoothed_val = last * weight + (1 - weight) * i\n        buffer.append(smoothed_val)\n        last = smoothed_val\n    return buffer\n \nnum_layers = 1\nsize_layer = 128\ntimestamp = 6\nepoch = 300\ndropout_rate = 0.8\nfuture_day = test_size\nlearning_rate = 0.01\n\ndef forecast():\n    model = Model(learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate)\n    minmax = MinMaxScaler().fit(df_log)\n    df_train = minmax.transform(df_log)\n    \n    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n\n    pbar = tqdm(range(epoch), desc='train loop')\n    for i in pbar:\n        init_value = np.zeros((1, num_layers * size_layer))\n        total_loss, total_acc = [], []\n        for k in range(0, df_train.shape[0] - 1, timestamp):\n            index = min(k + timestamp, df_train.shape[0] - 1)\n            batch_x = np.expand_dims(df_train[k:index, :], axis=0)\n            batch_y = df_train[k + 1 : index + 1, :]\n            with tf.GradientTape() as tape:\n                logits = model(batch_x)\n                loss = tf.reduce_mean(tf.square(batch_y - logits))\n            gradients = tape.gradient(loss, model.trainable_variables)\n            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n            total_loss.append(loss.numpy())\n            total_acc.append(calculate_accuracy(batch_y, logits))\n        pbar.set_postfix(cost=np.mean(total_loss), acc=np.mean(total_acc))\n    \n    future_day = test_size\n\n    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n    output_predict[0] = df_train.iloc[0]\n    upper_b = (df_train.shape[0] // timestamp) * timestamp\n    init_value = np.zeros((1, num_layers * 2 * size_layer))\n\n    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n        out_logits, last_state = sess.run(\n            [modelnn.logits, modelnn.last_state],\n            feed_dict = {\n                modelnn.X: np.expand_dims(\n                    df_train.iloc[k : k + timestamp], axis = 0\n                ),\n                modelnn.hidden_layer: init_value,\n            },\n        )\n        init_value = last_state\n        output_predict[k + 1 : k + timestamp + 1] = out_logits\n\n    if upper_b != df_train.shape[0]:\n        out_logits, last_state = sess.run(\n            [modelnn.logits, modelnn.last_state],\n            feed_dict = {\n                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n                modelnn.hidden_layer: init_value,\n            },\n        )\n        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n        future_day -= 1\n        date_ori.append(date_ori[-1] + timedelta(days = 1))\n\n    init_value = last_state\n    \n    for i in range(future_day):\n        o = output_predict[-future_day - timestamp + i:-future_day + i]\n        out_logits, last_state = sess.run(\n            [model.rnn, model.last_state],\n            feed_dict = {\n                model.X: np.expand_dims(o, axis = 0),\n                model.hidden_layer: init_value,\n            },\n        )\n        init_value = last_state\n        output_predict[-future_day + i] = out_logits[-1]\n        date_ori.append(date_ori[-1] + timedelta(days = 1))\n    \n    output_predict = minmax.inverse_transform(output_predict)\n    deep_future = anchor(output_predict, 0.3)\n    \n    return deep_future[-test_size:]\n\nresults = []\nfor i in range(simulation_size):\n    print('simulation %d'%(i + 1))\n    results.append(forecast())","outputsMetadata":{"0":{"height":37,"type":"stream"},"1":{"height":57,"type":"stream"}}},"id":"4db61fd9-6f61-4173-8b09-d51b4f53f141","cell_type":"code","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":"simulation 1\n"},{"output_type":"stream","name":"stderr","text":"train loop: 100%|██████████| 300/300 [15:19:14<00:00, 183.85s/it, acc=-inf, cost=0.0344]  \n"},{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 122\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(simulation_size):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 122\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[49], line 70\u001b[0m, in \u001b[0;36mforecast\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m future_day \u001b[38;5;241m=\u001b[39m test_size\n\u001b[1;32m     69\u001b[0m output_predict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((df_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m future_day, df_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 70\u001b[0m output_predict[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m upper_b \u001b[38;5;241m=\u001b[39m (df_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m timestamp) \u001b[38;5;241m*\u001b[39m timestamp\n\u001b[1;32m     72\u001b[0m init_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, num_layers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m size_layer))\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"],"ename":"AttributeError","evalue":"'numpy.ndarray' object has no attribute 'iloc'"}]},{"source":"accuracies = [calculate_accuracy(df['Close'].iloc[-test_size:].values, r) for r in results]\n\nplt.figure(figsize = (15, 5))\nfor no, r in enumerate(results):\n    plt.plot(r, label = 'forecast %d'%(no + 1))\nplt.plot(df['Close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\nplt.legend()\nplt.title('average accuracy: %.4f'%(np.mean(accuracies)))\nplt.show()","metadata":{"executionCancelledAt":1703917435987},"cell_type":"code","id":"006f16ba-a2ea-4274-ad29-646aa0357835","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}